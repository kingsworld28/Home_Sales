{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":622},"executionInfo":{"elapsed":14381,"status":"error","timestamp":1708625670305,"user":{"displayName":"Kiara Shannon","userId":"15511117068059343223"},"user_tz":360},"id":"a_KW73O2e3dw","outputId":"428d70d3-f7bd-4df4-e50d-15fc04725816"},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: command not found: apt-get\n","zsh:1: command not found: apt-get\n","zsh:1: command not found: wget\n","tar: Error opening archive: Failed to open 'spark-3.4.0-bin-hadoop3.tgz'\n"]}],"source":["import os\n","# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","# spark_version = 'spark-3.4.0'\n","spark_version = 'spark-3.4.0'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"2XbWNf1Te5fM","colab":{"base_uri":"https://localhost:8080/","height":378},"executionInfo":{"status":"error","timestamp":1708836845294,"user_tz":360,"elapsed":149,"user":{"displayName":"Kiara Shannon","userId":"15511117068059343223"}},"outputId":"71188c89-519a-4f94-8674-dcc7dd1e8d65"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'pyspark'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a1d0818b10e8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create a SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Import packages\n","from pyspark.sql import SparkSession\n","import time\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wOJqxG_RPSwp","colab":{"base_uri":"https://localhost:8080/","height":378},"executionInfo":{"status":"error","timestamp":1708836478461,"user_tz":360,"elapsed":170,"user":{"displayName":"Kiara Shannon","userId":"15511117068059343223"}},"outputId":"faa78ad7-de6d-46b4-b8c4-66ef26b2a309"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'pyspark'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-050267ad8dae>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. Read in the AWS S3 bucket into a DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# 1. Read in the AWS S3 bucket into a DataFrame.\n","from pyspark import SparkFiles\n","from google.colab import files\n","import pandas as pd\n","url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n","spark.sparkContext.addFile(url)\n","df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep=\",\", header=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RoljcJ7WPpnm"},"outputs":[],"source":["# 2. Create a temporary view of the DataFrame.\n","df.createOrReplaceTempView('home_sales')\n","\n","# Show DataFrame\n","df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6fkwOeOmqvq"},"outputs":[],"source":["# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n","avg_price = spark.sql((\"\"\"\n","    SELECT\n","      YEAR(date) AS year,\n","      CONCAT('$', format_number(ROUND(AVG(price), 2), '0,000.00')) AS price\n","    FROM\n","      home_sales\n","    WHERE\n","      bedrooms = 4\n","    GROUP BY\n","      YEAR(date)\n","    ORDER BY\n","      year DESC\n","\"\"\"))\n","avg_price.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8p_tUS8h8it"},"outputs":[],"source":["# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n","avg_price = spark.sql((\"\"\"\n","    SELECT\n","      YEAR(date) AS year,\n","      CONCAT('$', format_number(ROUND(AVG(price), 2), '0,000.00')) AS price\n","    FROM\n","      home_sales\n","    WHERE\n","      bedrooms = 3\n","      AND\n","      bathrooms = 3\n","    GROUP BY\n","      YEAR(date)\n","    ORDER BY\n","      year DESC\n","\"\"\"))\n","avg_price.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-Eytz64liDU"},"outputs":[],"source":["# 5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n","# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n","avg_price = spark.sql(\"\"\"\n","    SELECT\n","      YEAR(date) AS year,\n","      CONCAT('$', format_number(ROUND(AVG(price), 2), '0,000.00')) AS price\n","    FROM\n","      home_sales\n","    WHERE\n","      bedrooms = 3\n","      AND\n","      bathrooms = 3\n","      AND\n","      floors = 2\n","      AND\n","      sqft_living >= 2000\n","    GROUP BY\n","      YEAR(date)\n","    ORDER BY year DESC\n","\"\"\")\n","avg_price.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUrfgOX1pCRd","outputId":"0a645986-7179-440c-c0bc-345f1693f8af"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- 7.200241088867188e-05 seconds ---\n"]}],"source":["# 6. What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than\n","# or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n","\n","start_time = time.time()\n","\n","avg_view = spark.sql(\"\"\"\n","SELECT\n","    view,\n","    CONCAT('$', format_number(ROUND(AVG(price), 2), '0,000.00')) AS price\n","FROM\n","    home_sales\n","WHERE\n","    price >= 350000\n","GROUP BY\n","    view\n","ORDER BY\n","    CAST(view AS DOUBLE) DESC\n","\"\"\")\n","\n","avg_view.show()\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KAhk3ZD2tFy8"},"outputs":[],"source":["# 7. Cache the the temporary table home_sales.\n","\n","spark.sql(\"CACHE TABLE home_sales\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4opVhbvxtL-i"},"outputs":[],"source":["# 8. Check if the table is cached.\n","spark.catalog.isCached('home_sales')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5GnL46lwTSEk","outputId":"63c7dc50-d96a-4a48-97b6-91a446cdb973"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- 5.459785461425781e-05 seconds ---\n"]}],"source":["# 9. Using the cached data, run the query that filters out the view ratings with average price\n","#  greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n","\n","start_time = time.time()\n","\n","avg_view = spark.sql(\"\"\"\n","SELECT\n","    view,\n","    CONCAT('$', format_number(ROUND(AVG(price), 2), '0,000.00')) AS price\n","FROM\n","    home_sales\n","WHERE\n","    price >= 350000\n","GROUP BY\n","    view\n","ORDER BY\n","    CAST(view AS DOUBLE) DESC\n","\"\"\")\n","\n","avg_view.show()\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qm12WN9isHBR"},"outputs":[],"source":["# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n","df.write.partitionBy(\"date_built\").mode(\"overwrite\").parquet(\"homes_parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZ7BgY61sRqY"},"outputs":[],"source":["# 11. Read the parquet formatted data.\n","parquet_df=spark.read.parquet('homes_parquet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6MJkHfvVcvh"},"outputs":[],"source":["# 12. Create a temporary table for the parquet data.\n","parquet_df.createOrReplaceTempView('homes_parquet')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_Vhb52rU1Sn","outputId":"d6748ea6-d70a-41fd-dcb8-c214a85e949e"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- 7.104873657226562e-05 seconds ---\n"]}],"source":["# 13. Run the query that filters out the view ratings with average price of greater than or equal to $350,000\n","# with the parquet DataFrame. Round your average to two decimal places.\n","# Determine the runtime and compare it to the cached version.\n","\n","start_time = time.time()\n","\n","avg_view = spark.sql(\"\"\"\n","SELECT\n","    view,\n","    CONCAT('$', format_number(ROUND(AVG(price), 2), '0,000.00')) AS price\n","FROM\n","    homes_parquet\n","WHERE\n","    price >= 350000\n","GROUP BY\n","    view\n","ORDER BY\n","    CAST(view AS DOUBLE) DESC\n","\"\"\")\n","\n","avg_view.show()\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjjYzQGjtbq8"},"outputs":[],"source":["# 14. Uncache the home_sales temporary table.\n","spark.sql(\"uncache table home_sales\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sy9NBvO7tlmm"},"outputs":[],"source":["# 15. Check if the home_sales is no longer cached\n","spark.catalog.isCached('home_sales')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Si-BNruRUGK3"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"nteract":{"version":"0.28.0"}},"nbformat":4,"nbformat_minor":0}